# -*- coding: utf-8 -*-
"""SVR_ElectricityPricePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14NNQxTkOTgwQKZ9JcnUGPB42YbJQxTPf
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd
import pylab as pl
import numpy as np
# %matplotlib inline

"""SVR"""

from google.colab import drive
drive .mount('/content/drive')

data=pd.read_csv("/content/drive/MyDrive/electricity.csv")
data.head()

data.info()

#convert these string values to float values
data["ForecastWindProduction"] = pd.to_numeric(data["ForecastWindProduction"], errors= 'coerce')
data["SystemLoadEA"] = pd.to_numeric(data["SystemLoadEA"], errors= 'coerce')
data["SMPEA"] = pd.to_numeric(data["SMPEA"], errors= 'coerce')
data["ORKTemperature"] = pd.to_numeric(data["ORKTemperature"], errors= 'coerce')
data["ORKWindspeed"] = pd.to_numeric(data["ORKWindspeed"], errors= 'coerce')
data["CO2Intensity"] = pd.to_numeric(data["CO2Intensity"], errors= 'coerce')
data["ActualWindProduction"] = pd.to_numeric(data["ActualWindProduction"], errors= 'coerce')
data["SystemLoadEP2"] = pd.to_numeric(data["SystemLoadEP2"], errors= 'coerce')
data["SMPEP2"] = pd.to_numeric(data["SMPEP2"], errors= 'coerce')

data.isnull().sum()
#No of null values for each feature

data = data.dropna()

data.isnull().sum()

#the correlation between all the columns in the dataset
import seaborn as sns
import matplotlib.pyplot as plt
correlations = data.corr(method='pearson')
plt.figure(figsize=(16, 12))
sns.heatmap(correlations, cmap="coolwarm", annot=True)
plt.show()

x = data[["Day", "Month", "ForecastWindProduction", "SystemLoadEA",
          "SMPEA", "ORKTemperature", "ORKWindspeed", "CO2Intensity",
          "ActualWindProduction", "SystemLoadEP2"]]
y = data["SMPEP2"]

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y,test_size=0.2,random_state=42)

print('Shape of x_new training set {}'.format(xtrain.shape),'&',' Size of Y training set {}'.format(ytrain.shape))

print('Shape of X_new testing set {}'.format(xtest.shape),'&',' Size of Y testing set {}'.format(ytest.shape))

#Performing Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
# %matplotlib inline
import matplotlib.pyplot as plt

from sklearn import svm
model = svm.SVR()
model.fit( xtrain,ytrain)

y_hat=model.predict(xtest)

y_hat

# Commented out IPython magic to ensure Python compatibility.
x = xtest
y = ytest
print("Residual sum of squares: %.2f"
#       % np.mean((y_hat - y) ** 2))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % model.score(x, y))

from sklearn import metrics
from sklearn.metrics import mean_absolute_error
metrics.mean_absolute_error(y, y_hat)

from sklearn import metrics
from sklearn.metrics import mean_squared_error
metrics.mean_squared_error(y, y_hat)

features = np.array([[10, 12, 54.10, 4241.05, 49.56, 9.0, 14.8, 491.32, 54.0, 4426.84]])
model.predict(features)